// Code generated by the FlatBuffers compiler. DO NOT EDIT.

package flatbuf

import (
	flatbuffers "github.com/google/flatbuffers/go"
	"strconv"
)

type CompressionFormat int8

const (
	CompressionFormatNone   CompressionFormat = 0
	CompressionFormatSnappy CompressionFormat = 1
	CompressionFormatZlib   CompressionFormat = 2
	CompressionFormatLz4    CompressionFormat = 3
	CompressionFormatZstd   CompressionFormat = 4
)

var EnumNamesCompressionFormat = map[CompressionFormat]string{
	CompressionFormatNone:   "None",
	CompressionFormatSnappy: "Snappy",
	CompressionFormatZlib:   "Zlib",
	CompressionFormatLz4:    "Lz4",
	CompressionFormatZstd:   "Zstd",
}

var EnumValuesCompressionFormat = map[string]CompressionFormat{
	"None":   CompressionFormatNone,
	"Snappy": CompressionFormatSnappy,
	"Zlib":   CompressionFormatZlib,
	"Lz4":    CompressionFormatLz4,
	"Zstd":   CompressionFormatZstd,
}

func (v CompressionFormat) String() string {
	if s, ok := EnumNamesCompressionFormat[v]; ok {
		return s
	}
	return "CompressionFormat(" + strconv.FormatInt(int64(v), 10) + ")"
}

type CompactedSstId struct {
	_tab flatbuffers.Table
}

func GetRootAsCompactedSstId(buf []byte, offset flatbuffers.UOffsetT) *CompactedSstId {
	n := flatbuffers.GetUOffsetT(buf[offset:])
	x := &CompactedSstId{}
	x.Init(buf, n+offset)
	return x
}

func FinishCompactedSstIdBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.Finish(offset)
}

func GetSizePrefixedRootAsCompactedSstId(buf []byte, offset flatbuffers.UOffsetT) *CompactedSstId {
	n := flatbuffers.GetUOffsetT(buf[offset+flatbuffers.SizeUint32:])
	x := &CompactedSstId{}
	x.Init(buf, n+offset+flatbuffers.SizeUint32)
	return x
}

func FinishSizePrefixedCompactedSstIdBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.FinishSizePrefixed(offset)
}

func (rcv *CompactedSstId) Init(buf []byte, i flatbuffers.UOffsetT) {
	rcv._tab.Bytes = buf
	rcv._tab.Pos = i
}

func (rcv *CompactedSstId) Table() flatbuffers.Table {
	return rcv._tab
}

func (rcv *CompactedSstId) High() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *CompactedSstId) MutateHigh(n uint64) bool {
	return rcv._tab.MutateUint64Slot(4, n)
}

func (rcv *CompactedSstId) Low() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *CompactedSstId) MutateLow(n uint64) bool {
	return rcv._tab.MutateUint64Slot(6, n)
}

func CompactedSstIdStart(builder *flatbuffers.Builder) {
	builder.StartObject(2)
}
func CompactedSstIdAddHigh(builder *flatbuffers.Builder, high uint64) {
	builder.PrependUint64Slot(0, high, 0)
}
func CompactedSstIdAddLow(builder *flatbuffers.Builder, low uint64) {
	builder.PrependUint64Slot(1, low, 0)
}
func CompactedSstIdEnd(builder *flatbuffers.Builder) flatbuffers.UOffsetT {
	return builder.EndObject()
}

type CompactedSsTable struct {
	_tab flatbuffers.Table
}

func GetRootAsCompactedSsTable(buf []byte, offset flatbuffers.UOffsetT) *CompactedSsTable {
	n := flatbuffers.GetUOffsetT(buf[offset:])
	x := &CompactedSsTable{}
	x.Init(buf, n+offset)
	return x
}

func FinishCompactedSsTableBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.Finish(offset)
}

func GetSizePrefixedRootAsCompactedSsTable(buf []byte, offset flatbuffers.UOffsetT) *CompactedSsTable {
	n := flatbuffers.GetUOffsetT(buf[offset+flatbuffers.SizeUint32:])
	x := &CompactedSsTable{}
	x.Init(buf, n+offset+flatbuffers.SizeUint32)
	return x
}

func FinishSizePrefixedCompactedSsTableBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.FinishSizePrefixed(offset)
}

func (rcv *CompactedSsTable) Init(buf []byte, i flatbuffers.UOffsetT) {
	rcv._tab.Bytes = buf
	rcv._tab.Pos = i
}

func (rcv *CompactedSsTable) Table() flatbuffers.Table {
	return rcv._tab
}

func (rcv *CompactedSsTable) Id(obj *CompactedSstId) *CompactedSstId {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		x := rcv._tab.Indirect(o + rcv._tab.Pos)
		if obj == nil {
			obj = new(CompactedSstId)
		}
		obj.Init(rcv._tab.Bytes, x)
		return obj
	}
	return nil
}

func (rcv *CompactedSsTable) Info(obj *SsTableInfo) *SsTableInfo {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		x := rcv._tab.Indirect(o + rcv._tab.Pos)
		if obj == nil {
			obj = new(SsTableInfo)
		}
		obj.Init(rcv._tab.Bytes, x)
		return obj
	}
	return nil
}

func CompactedSsTableStart(builder *flatbuffers.Builder) {
	builder.StartObject(2)
}
func CompactedSsTableAddId(builder *flatbuffers.Builder, id flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(0, flatbuffers.UOffsetT(id), 0)
}
func CompactedSsTableAddInfo(builder *flatbuffers.Builder, info flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(1, flatbuffers.UOffsetT(info), 0)
}
func CompactedSsTableEnd(builder *flatbuffers.Builder) flatbuffers.UOffsetT {
	return builder.EndObject()
}

type SsTableInfo struct {
	_tab flatbuffers.Table
}

func GetRootAsSsTableInfo(buf []byte, offset flatbuffers.UOffsetT) *SsTableInfo {
	n := flatbuffers.GetUOffsetT(buf[offset:])
	x := &SsTableInfo{}
	x.Init(buf, n+offset)
	return x
}

func FinishSsTableInfoBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.Finish(offset)
}

func GetSizePrefixedRootAsSsTableInfo(buf []byte, offset flatbuffers.UOffsetT) *SsTableInfo {
	n := flatbuffers.GetUOffsetT(buf[offset+flatbuffers.SizeUint32:])
	x := &SsTableInfo{}
	x.Init(buf, n+offset+flatbuffers.SizeUint32)
	return x
}

func FinishSizePrefixedSsTableInfoBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.FinishSizePrefixed(offset)
}

func (rcv *SsTableInfo) Init(buf []byte, i flatbuffers.UOffsetT) {
	rcv._tab.Bytes = buf
	rcv._tab.Pos = i
}

func (rcv *SsTableInfo) Table() flatbuffers.Table {
	return rcv._tab
}

func (rcv *SsTableInfo) FirstKey(j int) byte {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		a := rcv._tab.Vector(o)
		return rcv._tab.GetByte(a + flatbuffers.UOffsetT(j*1))
	}
	return 0
}

func (rcv *SsTableInfo) FirstKeyLength() int {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		return rcv._tab.VectorLen(o)
	}
	return 0
}

func (rcv *SsTableInfo) FirstKeyBytes() []byte {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		return rcv._tab.ByteVector(o + rcv._tab.Pos)
	}
	return nil
}

func (rcv *SsTableInfo) MutateFirstKey(j int, n byte) bool {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		a := rcv._tab.Vector(o)
		return rcv._tab.MutateByte(a+flatbuffers.UOffsetT(j*1), n)
	}
	return false
}

func (rcv *SsTableInfo) IndexOffset() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *SsTableInfo) MutateIndexOffset(n uint64) bool {
	return rcv._tab.MutateUint64Slot(6, n)
}

func (rcv *SsTableInfo) IndexLen() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(8))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *SsTableInfo) MutateIndexLen(n uint64) bool {
	return rcv._tab.MutateUint64Slot(8, n)
}

func (rcv *SsTableInfo) FilterOffset() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(10))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *SsTableInfo) MutateFilterOffset(n uint64) bool {
	return rcv._tab.MutateUint64Slot(10, n)
}

func (rcv *SsTableInfo) FilterLen() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(12))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *SsTableInfo) MutateFilterLen(n uint64) bool {
	return rcv._tab.MutateUint64Slot(12, n)
}

func (rcv *SsTableInfo) CompressionFormat() CompressionFormat {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(14))
	if o != 0 {
		return CompressionFormat(rcv._tab.GetInt8(o + rcv._tab.Pos))
	}
	return 0
}

func (rcv *SsTableInfo) MutateCompressionFormat(n CompressionFormat) bool {
	return rcv._tab.MutateInt8Slot(14, int8(n))
}

func SsTableInfoStart(builder *flatbuffers.Builder) {
	builder.StartObject(6)
}
func SsTableInfoAddFirstKey(builder *flatbuffers.Builder, firstKey flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(0, flatbuffers.UOffsetT(firstKey), 0)
}
func SsTableInfoStartFirstKeyVector(builder *flatbuffers.Builder, numElems int) flatbuffers.UOffsetT {
	return builder.StartVector(1, numElems, 1)
}
func SsTableInfoAddIndexOffset(builder *flatbuffers.Builder, indexOffset uint64) {
	builder.PrependUint64Slot(1, indexOffset, 0)
}
func SsTableInfoAddIndexLen(builder *flatbuffers.Builder, indexLen uint64) {
	builder.PrependUint64Slot(2, indexLen, 0)
}
func SsTableInfoAddFilterOffset(builder *flatbuffers.Builder, filterOffset uint64) {
	builder.PrependUint64Slot(3, filterOffset, 0)
}
func SsTableInfoAddFilterLen(builder *flatbuffers.Builder, filterLen uint64) {
	builder.PrependUint64Slot(4, filterLen, 0)
}
func SsTableInfoAddCompressionFormat(builder *flatbuffers.Builder, compressionFormat CompressionFormat) {
	builder.PrependInt8Slot(5, int8(compressionFormat), 0)
}
func SsTableInfoEnd(builder *flatbuffers.Builder) flatbuffers.UOffsetT {
	return builder.EndObject()
}

type BlockMeta struct {
	_tab flatbuffers.Table
}

func GetRootAsBlockMeta(buf []byte, offset flatbuffers.UOffsetT) *BlockMeta {
	n := flatbuffers.GetUOffsetT(buf[offset:])
	x := &BlockMeta{}
	x.Init(buf, n+offset)
	return x
}

func FinishBlockMetaBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.Finish(offset)
}

func GetSizePrefixedRootAsBlockMeta(buf []byte, offset flatbuffers.UOffsetT) *BlockMeta {
	n := flatbuffers.GetUOffsetT(buf[offset+flatbuffers.SizeUint32:])
	x := &BlockMeta{}
	x.Init(buf, n+offset+flatbuffers.SizeUint32)
	return x
}

func FinishSizePrefixedBlockMetaBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.FinishSizePrefixed(offset)
}

func (rcv *BlockMeta) Init(buf []byte, i flatbuffers.UOffsetT) {
	rcv._tab.Bytes = buf
	rcv._tab.Pos = i
}

func (rcv *BlockMeta) Table() flatbuffers.Table {
	return rcv._tab
}

func (rcv *BlockMeta) Offset() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *BlockMeta) MutateOffset(n uint64) bool {
	return rcv._tab.MutateUint64Slot(4, n)
}

func (rcv *BlockMeta) FirstKey(j int) byte {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		a := rcv._tab.Vector(o)
		return rcv._tab.GetByte(a + flatbuffers.UOffsetT(j*1))
	}
	return 0
}

func (rcv *BlockMeta) FirstKeyLength() int {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		return rcv._tab.VectorLen(o)
	}
	return 0
}

func (rcv *BlockMeta) FirstKeyBytes() []byte {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		return rcv._tab.ByteVector(o + rcv._tab.Pos)
	}
	return nil
}

func (rcv *BlockMeta) MutateFirstKey(j int, n byte) bool {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		a := rcv._tab.Vector(o)
		return rcv._tab.MutateByte(a+flatbuffers.UOffsetT(j*1), n)
	}
	return false
}

func BlockMetaStart(builder *flatbuffers.Builder) {
	builder.StartObject(2)
}
func BlockMetaAddOffset(builder *flatbuffers.Builder, offset uint64) {
	builder.PrependUint64Slot(0, offset, 0)
}
func BlockMetaAddFirstKey(builder *flatbuffers.Builder, firstKey flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(1, flatbuffers.UOffsetT(firstKey), 0)
}
func BlockMetaStartFirstKeyVector(builder *flatbuffers.Builder, numElems int) flatbuffers.UOffsetT {
	return builder.StartVector(1, numElems, 1)
}
func BlockMetaEnd(builder *flatbuffers.Builder) flatbuffers.UOffsetT {
	return builder.EndObject()
}

type SsTableIndex struct {
	_tab flatbuffers.Table
}

func GetRootAsSsTableIndex(buf []byte, offset flatbuffers.UOffsetT) *SsTableIndex {
	n := flatbuffers.GetUOffsetT(buf[offset:])
	x := &SsTableIndex{}
	x.Init(buf, n+offset)
	return x
}

func FinishSsTableIndexBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.Finish(offset)
}

func GetSizePrefixedRootAsSsTableIndex(buf []byte, offset flatbuffers.UOffsetT) *SsTableIndex {
	n := flatbuffers.GetUOffsetT(buf[offset+flatbuffers.SizeUint32:])
	x := &SsTableIndex{}
	x.Init(buf, n+offset+flatbuffers.SizeUint32)
	return x
}

func FinishSizePrefixedSsTableIndexBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.FinishSizePrefixed(offset)
}

func (rcv *SsTableIndex) Init(buf []byte, i flatbuffers.UOffsetT) {
	rcv._tab.Bytes = buf
	rcv._tab.Pos = i
}

func (rcv *SsTableIndex) Table() flatbuffers.Table {
	return rcv._tab
}

func (rcv *SsTableIndex) BlockMeta(obj *BlockMeta, j int) bool {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		x := rcv._tab.Vector(o)
		x += flatbuffers.UOffsetT(j) * 4
		x = rcv._tab.Indirect(x)
		obj.Init(rcv._tab.Bytes, x)
		return true
	}
	return false
}

func (rcv *SsTableIndex) BlockMetaLength() int {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		return rcv._tab.VectorLen(o)
	}
	return 0
}

func SsTableIndexStart(builder *flatbuffers.Builder) {
	builder.StartObject(1)
}
func SsTableIndexAddBlockMeta(builder *flatbuffers.Builder, blockMeta flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(0, flatbuffers.UOffsetT(blockMeta), 0)
}
func SsTableIndexStartBlockMetaVector(builder *flatbuffers.Builder, numElems int) flatbuffers.UOffsetT {
	return builder.StartVector(4, numElems, 4)
}
func SsTableIndexEnd(builder *flatbuffers.Builder) flatbuffers.UOffsetT {
	return builder.EndObject()
}

type ManifestV1 struct {
	_tab flatbuffers.Table
}

func GetRootAsManifestV1(buf []byte, offset flatbuffers.UOffsetT) *ManifestV1 {
	n := flatbuffers.GetUOffsetT(buf[offset:])
	x := &ManifestV1{}
	x.Init(buf, n+offset)
	return x
}

func FinishManifestV1Buffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.Finish(offset)
}

func GetSizePrefixedRootAsManifestV1(buf []byte, offset flatbuffers.UOffsetT) *ManifestV1 {
	n := flatbuffers.GetUOffsetT(buf[offset+flatbuffers.SizeUint32:])
	x := &ManifestV1{}
	x.Init(buf, n+offset+flatbuffers.SizeUint32)
	return x
}

func FinishSizePrefixedManifestV1Buffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.FinishSizePrefixed(offset)
}

func (rcv *ManifestV1) Init(buf []byte, i flatbuffers.UOffsetT) {
	rcv._tab.Bytes = buf
	rcv._tab.Pos = i
}

func (rcv *ManifestV1) Table() flatbuffers.Table {
	return rcv._tab
}

func (rcv *ManifestV1) ManifestId() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *ManifestV1) MutateManifestId(n uint64) bool {
	return rcv._tab.MutateUint64Slot(4, n)
}

func (rcv *ManifestV1) WriterEpoch() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *ManifestV1) MutateWriterEpoch(n uint64) bool {
	return rcv._tab.MutateUint64Slot(6, n)
}

func (rcv *ManifestV1) CompactorEpoch() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(8))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *ManifestV1) MutateCompactorEpoch(n uint64) bool {
	return rcv._tab.MutateUint64Slot(8, n)
}

func (rcv *ManifestV1) WalIdLastCompacted() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(10))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *ManifestV1) MutateWalIdLastCompacted(n uint64) bool {
	return rcv._tab.MutateUint64Slot(10, n)
}

func (rcv *ManifestV1) WalIdLastSeen() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(12))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *ManifestV1) MutateWalIdLastSeen(n uint64) bool {
	return rcv._tab.MutateUint64Slot(12, n)
}

func (rcv *ManifestV1) L0LastCompacted(obj *CompactedSstId) *CompactedSstId {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(14))
	if o != 0 {
		x := rcv._tab.Indirect(o + rcv._tab.Pos)
		if obj == nil {
			obj = new(CompactedSstId)
		}
		obj.Init(rcv._tab.Bytes, x)
		return obj
	}
	return nil
}

func (rcv *ManifestV1) L0(obj *CompactedSsTable, j int) bool {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(16))
	if o != 0 {
		x := rcv._tab.Vector(o)
		x += flatbuffers.UOffsetT(j) * 4
		x = rcv._tab.Indirect(x)
		obj.Init(rcv._tab.Bytes, x)
		return true
	}
	return false
}

func (rcv *ManifestV1) L0Length() int {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(16))
	if o != 0 {
		return rcv._tab.VectorLen(o)
	}
	return 0
}

func (rcv *ManifestV1) Compacted(obj *SortedRun, j int) bool {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(18))
	if o != 0 {
		x := rcv._tab.Vector(o)
		x += flatbuffers.UOffsetT(j) * 4
		x = rcv._tab.Indirect(x)
		obj.Init(rcv._tab.Bytes, x)
		return true
	}
	return false
}

func (rcv *ManifestV1) CompactedLength() int {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(18))
	if o != 0 {
		return rcv._tab.VectorLen(o)
	}
	return 0
}

func (rcv *ManifestV1) Snapshots(obj *Snapshot, j int) bool {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(20))
	if o != 0 {
		x := rcv._tab.Vector(o)
		x += flatbuffers.UOffsetT(j) * 4
		x = rcv._tab.Indirect(x)
		obj.Init(rcv._tab.Bytes, x)
		return true
	}
	return false
}

func (rcv *ManifestV1) SnapshotsLength() int {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(20))
	if o != 0 {
		return rcv._tab.VectorLen(o)
	}
	return 0
}

func ManifestV1Start(builder *flatbuffers.Builder) {
	builder.StartObject(9)
}
func ManifestV1AddManifestId(builder *flatbuffers.Builder, manifestId uint64) {
	builder.PrependUint64Slot(0, manifestId, 0)
}
func ManifestV1AddWriterEpoch(builder *flatbuffers.Builder, writerEpoch uint64) {
	builder.PrependUint64Slot(1, writerEpoch, 0)
}
func ManifestV1AddCompactorEpoch(builder *flatbuffers.Builder, compactorEpoch uint64) {
	builder.PrependUint64Slot(2, compactorEpoch, 0)
}
func ManifestV1AddWalIdLastCompacted(builder *flatbuffers.Builder, walIdLastCompacted uint64) {
	builder.PrependUint64Slot(3, walIdLastCompacted, 0)
}
func ManifestV1AddWalIdLastSeen(builder *flatbuffers.Builder, walIdLastSeen uint64) {
	builder.PrependUint64Slot(4, walIdLastSeen, 0)
}
func ManifestV1AddL0LastCompacted(builder *flatbuffers.Builder, l0LastCompacted flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(5, flatbuffers.UOffsetT(l0LastCompacted), 0)
}
func ManifestV1AddL0(builder *flatbuffers.Builder, l0 flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(6, flatbuffers.UOffsetT(l0), 0)
}
func ManifestV1StartL0Vector(builder *flatbuffers.Builder, numElems int) flatbuffers.UOffsetT {
	return builder.StartVector(4, numElems, 4)
}
func ManifestV1AddCompacted(builder *flatbuffers.Builder, compacted flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(7, flatbuffers.UOffsetT(compacted), 0)
}
func ManifestV1StartCompactedVector(builder *flatbuffers.Builder, numElems int) flatbuffers.UOffsetT {
	return builder.StartVector(4, numElems, 4)
}
func ManifestV1AddSnapshots(builder *flatbuffers.Builder, snapshots flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(8, flatbuffers.UOffsetT(snapshots), 0)
}
func ManifestV1StartSnapshotsVector(builder *flatbuffers.Builder, numElems int) flatbuffers.UOffsetT {
	return builder.StartVector(4, numElems, 4)
}
func ManifestV1End(builder *flatbuffers.Builder) flatbuffers.UOffsetT {
	return builder.EndObject()
}

type SortedRun struct {
	_tab flatbuffers.Table
}

func GetRootAsSortedRun(buf []byte, offset flatbuffers.UOffsetT) *SortedRun {
	n := flatbuffers.GetUOffsetT(buf[offset:])
	x := &SortedRun{}
	x.Init(buf, n+offset)
	return x
}

func FinishSortedRunBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.Finish(offset)
}

func GetSizePrefixedRootAsSortedRun(buf []byte, offset flatbuffers.UOffsetT) *SortedRun {
	n := flatbuffers.GetUOffsetT(buf[offset+flatbuffers.SizeUint32:])
	x := &SortedRun{}
	x.Init(buf, n+offset+flatbuffers.SizeUint32)
	return x
}

func FinishSizePrefixedSortedRunBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.FinishSizePrefixed(offset)
}

func (rcv *SortedRun) Init(buf []byte, i flatbuffers.UOffsetT) {
	rcv._tab.Bytes = buf
	rcv._tab.Pos = i
}

func (rcv *SortedRun) Table() flatbuffers.Table {
	return rcv._tab
}

func (rcv *SortedRun) Id() uint32 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		return rcv._tab.GetUint32(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *SortedRun) MutateId(n uint32) bool {
	return rcv._tab.MutateUint32Slot(4, n)
}

func (rcv *SortedRun) Ssts(obj *CompactedSsTable, j int) bool {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		x := rcv._tab.Vector(o)
		x += flatbuffers.UOffsetT(j) * 4
		x = rcv._tab.Indirect(x)
		obj.Init(rcv._tab.Bytes, x)
		return true
	}
	return false
}

func (rcv *SortedRun) SstsLength() int {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		return rcv._tab.VectorLen(o)
	}
	return 0
}

func SortedRunStart(builder *flatbuffers.Builder) {
	builder.StartObject(2)
}
func SortedRunAddId(builder *flatbuffers.Builder, id uint32) {
	builder.PrependUint32Slot(0, id, 0)
}
func SortedRunAddSsts(builder *flatbuffers.Builder, ssts flatbuffers.UOffsetT) {
	builder.PrependUOffsetTSlot(1, flatbuffers.UOffsetT(ssts), 0)
}
func SortedRunStartSstsVector(builder *flatbuffers.Builder, numElems int) flatbuffers.UOffsetT {
	return builder.StartVector(4, numElems, 4)
}
func SortedRunEnd(builder *flatbuffers.Builder) flatbuffers.UOffsetT {
	return builder.EndObject()
}

type Snapshot struct {
	_tab flatbuffers.Table
}

func GetRootAsSnapshot(buf []byte, offset flatbuffers.UOffsetT) *Snapshot {
	n := flatbuffers.GetUOffsetT(buf[offset:])
	x := &Snapshot{}
	x.Init(buf, n+offset)
	return x
}

func FinishSnapshotBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.Finish(offset)
}

func GetSizePrefixedRootAsSnapshot(buf []byte, offset flatbuffers.UOffsetT) *Snapshot {
	n := flatbuffers.GetUOffsetT(buf[offset+flatbuffers.SizeUint32:])
	x := &Snapshot{}
	x.Init(buf, n+offset+flatbuffers.SizeUint32)
	return x
}

func FinishSizePrefixedSnapshotBuffer(builder *flatbuffers.Builder, offset flatbuffers.UOffsetT) {
	builder.FinishSizePrefixed(offset)
}

func (rcv *Snapshot) Init(buf []byte, i flatbuffers.UOffsetT) {
	rcv._tab.Bytes = buf
	rcv._tab.Pos = i
}

func (rcv *Snapshot) Table() flatbuffers.Table {
	return rcv._tab
}

func (rcv *Snapshot) Id() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(4))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *Snapshot) MutateId(n uint64) bool {
	return rcv._tab.MutateUint64Slot(4, n)
}

func (rcv *Snapshot) ManifestId() uint64 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(6))
	if o != 0 {
		return rcv._tab.GetUint64(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *Snapshot) MutateManifestId(n uint64) bool {
	return rcv._tab.MutateUint64Slot(6, n)
}

func (rcv *Snapshot) SnapshotExpireTimeS() uint32 {
	o := flatbuffers.UOffsetT(rcv._tab.Offset(8))
	if o != 0 {
		return rcv._tab.GetUint32(o + rcv._tab.Pos)
	}
	return 0
}

func (rcv *Snapshot) MutateSnapshotExpireTimeS(n uint32) bool {
	return rcv._tab.MutateUint32Slot(8, n)
}

func SnapshotStart(builder *flatbuffers.Builder) {
	builder.StartObject(3)
}
func SnapshotAddId(builder *flatbuffers.Builder, id uint64) {
	builder.PrependUint64Slot(0, id, 0)
}
func SnapshotAddManifestId(builder *flatbuffers.Builder, manifestId uint64) {
	builder.PrependUint64Slot(1, manifestId, 0)
}
func SnapshotAddSnapshotExpireTimeS(builder *flatbuffers.Builder, snapshotExpireTimeS uint32) {
	builder.PrependUint32Slot(2, snapshotExpireTimeS, 0)
}
func SnapshotEnd(builder *flatbuffers.Builder) flatbuffers.UOffsetT {
	return builder.EndObject()
}
